{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEOSNAP2NAM start at 2020-10-03 01:22:05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading manifest: 100%|██████████| 5/5 [00:00<00:00, 9387.43entries/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed package 'census/tracts_cartographic', tophash=476770c from s3://spatial-ucr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading manifest: 100%|██████████| 5/5 [00:00<00:00, 3436.83entries/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed package 'census/administrative', tophash=0616d02 from s3://spatial-ucr\n",
      "GEOSNAP2NAM ended at 2020-10-03 01:22:09    Elapsed 00:00:04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "import json, math, copy, sys\n",
    "from geosnap.io import store_ltdb\n",
    "from geosnap import Community, datasets\n",
    "from geosnap.io import store_census\n",
    "\n",
    "import pandas as pd\n",
    "import shapely.wkt\n",
    "import shapely.geometry\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from pathlib import Path\n",
    "from INCS import linc\n",
    "import urllib.parse\n",
    "import webbrowser\n",
    "import os\n",
    "import pprint\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from notebook import notebookapp\n",
    "\n",
    "# Print iterations progress\n",
    "def printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = '█', printEnd = \"\\r\"):\n",
    "\t\"\"\"\n",
    "\tCall in a loop to create terminal progress bar\n",
    "\t@params:\n",
    "\t\titeration   - Required  : current iteration (Int)\n",
    "\t\ttotal       - Required  : total iterations (Int)\n",
    "\t\tprefix      - Optional  : prefix string (Str)\n",
    "\t\tsuffix      - Optional  : suffix string (Str)\n",
    "\t\tdecimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "\t\tlength      - Optional  : character length of bar (Int)\n",
    "\t\tfill        - Optional  : bar fill character (Str)\n",
    "\t\tprintEnd    - Optional  : end character (e.g. \"\\r\", \"\\r\\n\") (Str)\n",
    "\t\"\"\"\n",
    "\tpercent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "\tfilledLength = int(length * iteration // total)\n",
    "\tbar = fill * filledLength + '-' * (length - filledLength)\n",
    "\tprint('\\r%s |%s| %s%% %s' % (prefix, bar, percent, suffix), end = printEnd)\n",
    "\t# Print New Line on Complete\n",
    "\tif iteration == total: \n",
    "\t\tprint()\n",
    "\n",
    "\n",
    "def write_LOG(param):\n",
    "\t#Create a new folder where GEO_CONFIG.js GEO_JSON.js VARIABLES.js will be saved\n",
    "\toDir = 'NAM_' + param['filename_suffix']\n",
    "\tpath = Path(oDir + '/data')\n",
    "\tpath.mkdir(parents=True, exist_ok=True)\n",
    "\t\n",
    "\tcontents = pprint.pformat(param)\n",
    "\t#print(oDir+\"/data/param.log\")\n",
    "\t#print(contents)\n",
    "\t#write new outfiles: GEO_CONFIG.js GEO_JSON.js VARIABLES.js\n",
    "\tofile = open(oDir+\"/data/param.log\", \"w\")\n",
    "\tcreate_at = datetime.now()\n",
    "\tofile.write('%s %s\\r\\n' % (create_at.strftime('%Y-%m-%d'), create_at.strftime('%H:%M:%S')))\n",
    "\t#ofile.write('\\r\\n\\r\\n')\n",
    "\tofile.write('  '+contents.replace('\\n', '\\n  '))\n",
    "\tofile.close()\n",
    "\n",
    "\n",
    "def write_INDEX_html(param):\n",
    "\t#Create a new folder where GEO_CONFIG.js GEO_JSON.js VARIABLES.js will be saved\n",
    "\toDir = 'NAM_' + param['filename_suffix']\n",
    "\tpath = Path(oDir + '/data')\n",
    "\tpath.mkdir(parents=True, exist_ok=True)\n",
    "\t\n",
    "\tcontents = []\n",
    "\t#open Neighborhood_Analysis_Mapper.html (the excutable file for the visualization)\n",
    "\tifile = open(\"template/Neighborhood_Analysis_Mapper.html\", \"r\", encoding=\"utf-8\")\n",
    "\tcontents = ifile.read()\n",
    "\t\n",
    "\t#Replace variables based on the user's selection in each of four files below.\n",
    "\tcontents = contents.replace(\"Neighborhood Analysis Mapper\", param['title'])\n",
    "\tcontents = contents.replace(\"data/GEO_CONFIG.js\", \"data/GEO_CONFIG_\"+param['filename_suffix']+\".js\")\n",
    "\tcontents = contents.replace(\"data/GEO_JSON.js\", \"data/GEO_JSON_\"+param['filename_suffix']+\".js\")\n",
    "\tcontents = contents.replace(\"data/GEO_VARIABLES.js\", \"data/GEO_VARIABLES_\"+param['filename_suffix']+\".js\")\n",
    "\t\n",
    "\t#write new outfiles: GEO_CONFIG.js GEO_JSON.js VARIABLES.js\n",
    "\tofile = open(oDir+\"/index.html\", \"w\", encoding=\"utf-8\")\n",
    "\tofile.write(contents)\n",
    "\tofile.close()\n",
    "\n",
    "\n",
    "def write_ALL_METROS_INDEX_html(param):\n",
    "\t#Create a new folder where GEO_CONFIG.js GEO_JSON.js VARIABLES.js will be saved\n",
    "\toDir = 'NAM_' + param['filename_suffix']\n",
    "\tpath = Path(oDir + '/data')\n",
    "\tpath.mkdir(parents=True, exist_ok=True)\n",
    "\t\n",
    "\tcontents = []\n",
    "\t#open Adaptive_Choropleth_Mapper.html (the excutable file for the visualization)\n",
    "\tifile = open(\"template/Adaptive_Choropleth_Mapper.html\", \"r\", encoding=\"utf-8\" )\n",
    "\tcontents = ifile.read()\n",
    "\t\n",
    "\t#Replace variables based on the user's selection in each of four files below.\n",
    "\tcontents = contents.replace(\"Adaptive Choropleth Mapper\", param['title'])\n",
    "\tcontents = contents.replace(\"data/GEO_CONFIG.js\", \"data/GEO_CONFIG_\"+param['filename_suffix']+\".js\")\n",
    "\tcontents = contents.replace(\"data/GEO_JSON.js\", \"data/GEO_JSON_\"+param['filename_suffix']+\".js\")\n",
    "\tcontents = contents.replace(\"data/GEO_VARIABLES.js\", \"data/GEO_VARIABLES_\"+param['filename_suffix']+\".js\")\n",
    "\t\n",
    "\t#write new outfiles: GEO_CONFIG.js GEO_JSON.js VARIABLES.js\n",
    "\tofile = open(oDir+\"/index.html\", \"w\", encoding=\"utf-8\")\n",
    "\tofile.write(contents)\n",
    "\tofile.close()\n",
    "\n",
    "\n",
    "def write_GEO_CONFIG_js(param):\n",
    "\t# read ACM_GEO_CONFIG.js\n",
    "\tifile = open(\"template/NAM_GEO_CONFIG.js\", \"r\", encoding=\"utf-8\")\n",
    "\tcontents = ifile.read()\n",
    "\t\n",
    "\tallMetros = False;\n",
    "\tIndex_of_neighborhood_change = True;\n",
    "\tMaps_of_neighborhood = True;               \n",
    "\tDistribution_INC1 = True;                  \n",
    "\tDistribution_period = False; \n",
    "\tDistribution_cluster = False;\n",
    "\tTemporal_change_in_neighborhoods = True;\n",
    "\tParallel_Categories_Diagram_in_neighborhoods = True;\n",
    "\tChord_Diagram_in_neighborhoods = True;\n",
    "\tZscore_Means_across_Clusters = True;\n",
    "\tZscore_Means_of_Each_Cluster = True;\n",
    "\t\n",
    "\tif ('allMetros' in param): allMetros =  param['allMetros']\n",
    "\tif ('Index_of_neighborhood_change' in param): Index_of_neighborhood_change =  param['Index_of_neighborhood_change']\n",
    "\tif ('Maps_of_neighborhood' in param): Maps_of_neighborhood =  param['Maps_of_neighborhood']\n",
    "\tif ('Distribution_INC1' in param): Distribution_INC1 =  param['Distribution_INC1']\n",
    "\tif ('Distribution_INC2_different_period' in param): Distribution_period =  param['Distribution_INC2_different_period']\n",
    "\tif ('Distribution_INC2_different_cluster' in param): Distribution_cluster =  param['Distribution_INC2_different_cluster']\n",
    "\tif ('Temporal_change_in_neighborhoods' in param): Temporal_change_in_neighborhoods =  param['Temporal_change_in_neighborhoods']\n",
    "\tif ('Parallel_Categories_Diagram_in_neighborhoods' in param): Parallel_Categories_Diagram_in_neighborhoods =  param['Parallel_Categories_Diagram_in_neighborhoods']\n",
    "\tif ('Chord_Diagram_in_neighborhoods' in param): Chord_Diagram_in_neighborhoods =  param['Chord_Diagram_in_neighborhoods']\n",
    "\tif ('Zscore_Means_across_Clusters' in param): Zscore_Means_across_Clusters =  param['Zscore_Means_across_Clusters']\n",
    "\tif ('Zscore_Means_of_Each_Cluster' in param): Zscore_Means_of_Each_Cluster =  param['Zscore_Means_of_Each_Cluster']\n",
    "\t\n",
    "\t# perpare parameters\n",
    "\t#NumOfMaps = len(param['years']) + 1\n",
    "\tNumOfMaps = len(param['years']) + 1 if Index_of_neighborhood_change else len(param['years'])\n",
    "\t#InitialLayers = [\"INC\"]\n",
    "\tInitialLayers = [\"INC\"] if Index_of_neighborhood_change else []\n",
    "\tif (len(param['years']) <= 1): InitialLayers = []\n",
    "\tfor i, year in enumerate(param['years']):\n",
    "\t\tInitialLayers.append(str(year))\n",
    "\t\n",
    "\t# Automatically set Map_width, Map_height. \n",
    "\tMap_width = \"300px\"\n",
    "\tMap_height = \"300px\"\n",
    "\tif (NumOfMaps <= 6):\n",
    "\t\tMap_width = \"300px\"\n",
    "\t\tMap_height = \"300px\"\t\n",
    "\tif (NumOfMaps <= 5):\n",
    "\t\tMap_width = \"350px\"\n",
    "\t\tMap_height = \"350px\"\n",
    "\tif (NumOfMaps <= 4):\n",
    "\t\tMap_width = \"400px\"\n",
    "\t\tMap_height = \"400px\"\n",
    "\tif (NumOfMaps <= 3):\n",
    "\t\tMap_width = \"400px\"\n",
    "\t\tMap_height = \"400px\"\n",
    "\tif (NumOfMaps <= 2):\n",
    "\t\tMap_width = \"450px\"\n",
    "\t\tMap_height = \"450px\"\n",
    "\tif (NumOfMaps ==\t1):\n",
    "\t\tMap_width = \"800px\"\n",
    "\t\tMap_height = \"800px\"\t\n",
    "\t# replace newly computed \"NumOfMaps\", \"InitialLayers\", \"Map_width\", \"Map_height\" in CONFIG.js. See the example replacement below\n",
    "\t'''\n",
    "\t\t'years': [1980, 1990, 2000, 2010]            ->    'var InitialLayers = [\"INC\", \"1980\", \"1990\", \"2000\", \"2010\"];'\n",
    "\t'''\n",
    "\tNumOfMaps = \"var NumOfMaps = \" + str(NumOfMaps) + \";\"\n",
    "\tInitialLayers = \"var InitialLayers = \" + json.dumps(InitialLayers) + \";\"\n",
    "\tallMetros = \"var allMetros = \" + json.dumps(allMetros)+ \";\"\n",
    "\tIndex_of_neighborhood_change = \"var Index_of_neighborhood_change = \" + json.dumps(Index_of_neighborhood_change)+ \";\"\n",
    "\tMaps_of_neighborhood = \"var Maps_of_neighborhood = \" + json.dumps(Maps_of_neighborhood)+ \";\"\n",
    "\tDistribution_INC1 = \"var Distribution_INC1 = \" + json.dumps(Distribution_INC1)+ \";\"\n",
    "\tDistribution_period = \"var Distribution_INC2_different_period = \" + json.dumps(Distribution_period)+ \";\"\n",
    "\tDistribution_cluster = \"var Distribution_INC2_different_cluster = \" + json.dumps(Distribution_cluster)+ \";\"\n",
    "\tTemporal_change_in_neighborhoods = \"var Temporal_change_in_neighborhoods = \" + json.dumps(Temporal_change_in_neighborhoods)+ \";\"\n",
    "\tParallel_Categories_Diagram_in_neighborhoods = \"var Parallel_Categories_Diagram_in_neighborhoods = \" + json.dumps(Parallel_Categories_Diagram_in_neighborhoods)+ \";\"\n",
    "\tChord_Diagram_in_neighborhoods = \"var Chord_Diagram_in_neighborhoods = \" + json.dumps(Chord_Diagram_in_neighborhoods)+ \";\"\n",
    "\tZscore_Means_across_Clusters = \"var Zscore_Means_across_Clusters = \" + json.dumps(Zscore_Means_across_Clusters)+ \";\"\n",
    "\tZscore_Means_of_Each_Cluster = \"var Zscore_Means_of_Each_Cluster = \" + json.dumps(Zscore_Means_of_Each_Cluster)+ \";\"\n",
    "\tMap_width = 'var Map_width  = \"' + Map_width + '\";'\n",
    "\tMap_height = 'var Map_height = \"' + Map_height + '\";'\n",
    "   \n",
    "\tcontents = contents.replace(\"var InitialLayers = [];\", InitialLayers)\n",
    "\tcontents = contents.replace(\"var allMetros = false;\", allMetros)\n",
    "\tcontents = contents.replace(\"var Index_of_neighborhood_change = true;\", Index_of_neighborhood_change)\n",
    "\tcontents = contents.replace(\"var Maps_of_neighborhood = true;\", Maps_of_neighborhood)\n",
    "\tcontents = contents.replace(\"var Distribution_INC1 = true;\", Distribution_INC1)\n",
    "\tcontents = contents.replace(\"var Distribution_INC2_different_period = true;\", Distribution_period)\n",
    "\tcontents = contents.replace(\"var Distribution_INC2_different_cluster = true;\", Distribution_cluster)\n",
    "\tcontents = contents.replace(\"var Temporal_change_in_neighborhoods = true;\", Temporal_change_in_neighborhoods)\n",
    "\tcontents = contents.replace(\"var Parallel_Categories_Diagram_in_neighborhoods = true;\", Parallel_Categories_Diagram_in_neighborhoods)\n",
    "\tcontents = contents.replace(\"var Chord_Diagram_in_neighborhoods = true;\", Chord_Diagram_in_neighborhoods)\n",
    "\tcontents = contents.replace(\"var Zscore_Means_across_Clusters = true;\", Zscore_Means_across_Clusters)\n",
    "\tcontents = contents.replace(\"var Zscore_Means_of_Each_Cluster = true;\", Zscore_Means_of_Each_Cluster)\n",
    "\tcontents = contents.replace('var Map_width  = \"400px\";', Map_width)\n",
    "\tcontents = contents.replace('var Map_height = \"400px\";', Map_height)\n",
    "\n",
    "\t#Write output including the replacement above\n",
    "\tfilename_GEO_CONFIG = \"NAM_\" + param['filename_suffix'] + \"/data/GEO_CONFIG_\"+param['filename_suffix']+\".js\"\n",
    "\tofile = open(filename_GEO_CONFIG, 'w', encoding=\"utf-8\")\n",
    "\tofile.write(contents)\n",
    "\tofile.close()\n",
    "\n",
    "\n",
    "def write_ALL_METROS_GEO_CONFIG_js(param):\n",
    "\t# read GEO_CONFIG.js\n",
    "\tifile = open(\"template/ACM_GEO_CONFIG.js\", \"r\", encoding=\"utf-8\")\n",
    "\tcontents = ifile.read()\n",
    "\t\n",
    "\tStacked_Chart = False;\n",
    "\tCorrelogram = False;\n",
    "\tScatter_Plot = False;               \n",
    "\tParallel_Coordinates_Plot = False;                  \n",
    "\t\n",
    "\t# perpare parameters\n",
    "\tNumOfMaps = 1\n",
    "\tInitialLayers = [\"INC\"]\n",
    "\t\n",
    "\t# Automatically set Map_width, Map_height. \n",
    "\tMap_width = \"1400px\"\n",
    "\tMap_height = \"800px\"\n",
    "\t\n",
    "\t# replace newly computed \"NumOfMaps\", \"InitialLayers\", \"Map_width\", \"Map_height\" in CONFIG.js. See the example replacement below\n",
    "\tNumOfMaps = \"var NumOfMaps = \" + str(NumOfMaps) + \";\"\n",
    "\tInitialLayers = \"var InitialLayers = \" + json.dumps(InitialLayers) + \";\"\n",
    "\tInitial_map_center = \"var Initial_map_center = [37, -98.5795];\"\n",
    "\tInitial_map_zoom_level = \"var Initial_map_zoom_level = 5;\"\n",
    "\tMap_width = 'var Map_width  = \"' + Map_width + '\";'\n",
    "\tMap_height = 'var Map_height = \"' + Map_height + '\";'\n",
    "   \n",
    "\tcontents = contents.replace(\"var NumOfMaps = 1;\", NumOfMaps)\n",
    "\tcontents = contents.replace(\"var InitialLayers = [];\", InitialLayers)\n",
    "\tcontents = contents.replace(\"//var Initial_map_center = [34.0522, -117.9];\", Initial_map_center)\n",
    "\tcontents = contents.replace(\"//var Initial_map_zoom_level = 8;\", Initial_map_zoom_level)\n",
    "\tcontents = contents.replace('var Map_width  = \"400px\";', Map_width)\n",
    "\tcontents = contents.replace('var Map_height = \"400px\";', Map_height)\n",
    "\n",
    "\t#Write output including the replacement above\n",
    "\tfilename_GEO_CONFIG = \"NAM_\" + param['filename_suffix'] + \"/data/GEO_CONFIG_\"+param['filename_suffix']+\".js\"\n",
    "\tofile = open(filename_GEO_CONFIG, 'w', encoding=\"utf-8\")\n",
    "\tofile.write(contents)\n",
    "\tofile.close()\n",
    "\n",
    "def write_GEO_JSON_js(community, param):\n",
    "\t# query geometry for each tract\n",
    "\tgeoid = community.gdf.columns[0]\n",
    "\ttracts = community.gdf[[geoid, 'geometry']].copy()\n",
    "\ttracts.drop_duplicates(subset=geoid, inplace=True)\t\t\t\t\t# get unique geoid\n",
    "\t#print(tracts)\n",
    "\t\n",
    "\t# open GEO_JSON.js write heading for geojson format\n",
    "\tfilename_GEO_JSON = \"NAM_\" + param['filename_suffix'] + \"/data/GEO_JSON_\"+param['filename_suffix']+\".js\"\n",
    "\tofile = open(filename_GEO_JSON, 'w')\n",
    "\tofile.write('var GEO_JSON =\\n')\n",
    "\tofile.write('{\"type\":\"FeatureCollection\", \"features\": [\\n')\n",
    "\t\n",
    "\t#Convert geometry in GEOJSONP to geojson format\n",
    "\twCount = 0\n",
    "\tfor tract in tracts.itertuples():\n",
    "\t\tfeature = {\"type\":\"Feature\"}\n",
    "\t\tif (type(tract.geometry) is float):\t\t\t\t\t\t\t\t# check is NaN?\n",
    "\t\t\t#print(tract.geometry)\n",
    "\t\t\tcontinue\n",
    "\t\tfeature[\"geometry\"] = shapely.geometry.mapping(tract.geometry)\n",
    "\t\t#feature[\"properties\"] = {geoid: tract.__getattribute__(geoid), \"tractID\": tract.__getattribute__(geoid)}\n",
    "\t\tfeature[\"properties\"] = {geoid: tract.__getattribute__(geoid)}\n",
    "\t\twCount += 1\n",
    "\t\tofile.write(json.dumps(feature)+',\\n')\n",
    "\t#print(\"GEO_JSON.js write count:\", wCount)\n",
    "\t# complete the geojosn format by adding parenthesis at the end.\t\n",
    "\tofile.write(']}\\n')\n",
    "\tofile.close()\n",
    "\n",
    "def write_ALL_METROS_JSON_js(metros, param):\n",
    "\t# query geometry for each tract\n",
    "\tgeoid = metros.columns[0]\n",
    "\ttracts = metros[[geoid, 'name', 'geometry']].copy()\n",
    "\ttracts.drop_duplicates(subset=geoid, inplace=True)\t\t\t\t\t# get unique geoid\n",
    "\t#print(tracts)\n",
    "\t\n",
    "\t# open GEO_JSON.js write heading for geojson format\n",
    "\tfilename_GEO_JSON = \"NAM_\" + param['filename_suffix'] + \"/data/GEO_JSON_\"+param['filename_suffix']+\".js\"\n",
    "\tofile = open(filename_GEO_JSON, 'w')\n",
    "\tofile.write('var GEO_JSON =\\n')\n",
    "\tofile.write('{\"type\":\"FeatureCollection\", \"features\": [\\n')\n",
    "\t\n",
    "\t#Convert geometry in GEOJSONP to geojson format\n",
    "\twCount = 0\n",
    "\tfor tract in tracts.itertuples():\n",
    "\t\tfeature = {\"type\":\"Feature\"}\n",
    "\t\tif (type(tract.geometry) is float):\t\t\t\t\t\t\t\t# check is NaN?\n",
    "\t\t\t#print(tract.geometry)\n",
    "\t\t\tcontinue\n",
    "\t\tfeature[\"geometry\"] = shapely.geometry.mapping(tract.geometry)\n",
    "\t\t#feature[\"properties\"] = {geoid: tract.__getattribute__(geoid), \"tractID\": tract.__getattribute__(geoid)}\n",
    "\t\tfeature[\"properties\"] = {geoid: tract.geoid, 'metro': tract.name}\n",
    "\t\twCount += 1\n",
    "\t\tofile.write(json.dumps(feature)+',\\n')\n",
    "\t#print(\"GEO_JSON.js write count:\", wCount)\n",
    "\t# complete the geojosn format by adding parenthesis at the end.\t\n",
    "\tofile.write(']}\\n')\n",
    "\tofile.close()\n",
    "\n",
    "\n",
    "def write_GEO_VARIABLES_js(community, param):\n",
    "\t#print(param)\n",
    "\tgeoid       = community.gdf.columns[0]\n",
    "\tmethod      = param['method']\n",
    "\tnClusters   = param['nClusters']\n",
    "\tyears       = param['years']\n",
    "\tvariables   = param['variables']\n",
    "\tlabels      = param['labels']\n",
    "\t\n",
    "\tseqClusters = 5\n",
    "\tdistType    = 'tran'\n",
    "\tif ('Sequence' in param and type(param['Sequence']) is dict):\n",
    "\t\tif ('seq_clusters' in param['Sequence']): seqClusters = param['Sequence']['seq_clusters']\n",
    "\t\tif ('dist_type' in param['Sequence']): distType = param['Sequence']['dist_type']\n",
    "\t\n",
    "\t# filtering by years\n",
    "\tcommunity.gdf = community.gdf[community.gdf.year.isin(years)]\n",
    "\t#print(community.gdf)\n",
    "\tcommunity.gdf.to_csv(r'output.csv')    \n",
    "\n",
    "\tif (method == 'kmeans' or method == 'ward' or method == 'affinity_propagation' or method == 'spectral' or method == 'gaussian_mixture' or method == 'hdbscan'):\n",
    "\t\tclusters = community.cluster(columns=variables, method=method, n_clusters=nClusters)\n",
    "\tif (method == 'ward_spatial' or method == 'spenc' or method == 'skater' or method == 'azp' or method == 'max_p'):\n",
    "\t\tclusters = community.cluster_spatial(columns=variables, method=method, n_clusters=nClusters)\t\t\n",
    "\n",
    "\t# Use the sequence method to obtain the distance matrix of neighborhood sequences\n",
    "\tgdf_new, df_wide, seq_dis_mat = clusters.sequence(seq_clusters=seqClusters, dist_type=distType, cluster_col=method)\n",
    "\t#print(df_wide)\n",
    "\t\n",
    "\t# pivot by year column\n",
    "\t#df_pivot = df.reset_index().pivot(geoid, \"year\", method)\n",
    "\tdf_pivot = df_wide\n",
    "\tlastColumn = df_pivot.columns[df_pivot.shape[1]-1]\t\t\t\t\t# get the last column name as like 'tran-5'\n",
    "\tdf_pivot.rename(columns={lastColumn: 'Sequence'}, inplace=True)\t\t# change the last column name to 'Sequence'\n",
    "\t#print(df_pivot)\n",
    "\t\n",
    "\tif (len(years) > 1):\n",
    "\t\t# convert df_pivot to list for INCS.linc\n",
    "\t\tyearList = []\n",
    "\t\t#for year in df_pivot.columns:\n",
    "\t\tfor year in years:\n",
    "\t\t\taYearList = df_pivot[year].values.tolist()\n",
    "\t\t\taYearList = list(map(float, aYearList)) \n",
    "\t\t\tyearList.append(aYearList)\n",
    "\t\t#print(yearList)\n",
    "\t\t# calculate INC\n",
    "\t\tincs = linc(yearList)\n",
    "\t\t#print(incs)\n",
    "\t\t#incs = minmax_scale(incs, feature_range=(0,1), axis=0)\n",
    "\t\t#print(incs)\t\t\n",
    "\t\t# insert INC to first column of df_pivot\n",
    "\t\tdf_pivot.insert(loc=0, column='INC', value=incs)\n",
    "\t\n",
    "\tif ('Sequence' not in param or not param['Sequence']): df_pivot.drop(columns=['Sequence'], inplace=True)\n",
    "\t#if ('Sequence' not in param or type(param['Sequence']) is not dict): df_pivot.drop(columns=['Sequence'], inplace=True)\n",
    "\t#print(df_pivot)\n",
    "\t\n",
    "\t# calculate zscore\n",
    "\tclusters_flattened = pd.DataFrame(df_pivot.to_records())                   # convert pivot to data frame\n",
    "\tgeoids = clusters_flattened[\"geoid\"].tolist()                              # get list of geoids from pivot\n",
    "\tvalid_gdf = community.gdf[community.gdf.geoid.isin(geoids)]                # get all rows of valid geoids from community.gdf\n",
    "\t#print(\"clusters_flattened:\", clusters_flattened)\n",
    "\t#print(\"geoids: \", len(geoids))\n",
    "\t#print(\"geoids:\", geoids)\n",
    "\t#print(\"valid_gdf:\", valid_gdf)\n",
    "\t\n",
    "\tlastClusterNo = 0\n",
    "\tfor y, year in enumerate(years):\n",
    "\t\tmaxClusterNo_theYear = clusters_flattened[str(year)].max()\n",
    "\t\tif (lastClusterNo < maxClusterNo_theYear): \n",
    "\t\t\tlastClusterNo = maxClusterNo_theYear\n",
    "\t#print(\"lastClusterNo:\", lastClusterNo)\n",
    "\tnGeneratedClusters = lastClusterNo + 1\n",
    "\t\n",
    "\t# get sum of the each cluster and count of the each cluster\n",
    "\tzValue = [[0 for v in range(len(variables))] for c in range(nGeneratedClusters)]\n",
    "\tzCount = [[0 for v in range(len(variables))] for c in range(nGeneratedClusters)]\n",
    "\tfor v, variable in enumerate(variables):\n",
    "\t\t#if (v > 0): break\n",
    "\t\ttheVariable_pivot = valid_gdf.pivot(index='geoid', columns='year', values=variable)\n",
    "\t\ttheVariable_flattened = pd.DataFrame(theVariable_pivot.to_records())   # convert pivot to data frame\n",
    "\t\t#print(theVariable_pivot)\n",
    "\t\t#print(\"theVariable_flattened: \", variable)\n",
    "\t\t#print(theVariable_flattened)\n",
    "\t\tif (theVariable_flattened.shape[0] != len(geoids)):                    # check number of pivot and valid geoids\n",
    "\t\t\tprint(\"Number of valid geoid not equal pivot of '\" + variable +\"'\")\n",
    "\t\t\tprint(\"Number of geoids: \", len(geoids))\n",
    "\t\t\tprint(\"Number of pivot : \", theVariable_flattened.shape[0])\n",
    "\t\t\tcontinue\n",
    "\t\t\n",
    "\t\t# make a variable list of all years from pivot\n",
    "\t\ttheVariableList = np.array([])\n",
    "\t\tfor y, year in enumerate(years):\n",
    "\t\t\ttheYearList = theVariable_flattened[str(year)].tolist()\n",
    "\t\t\ttheVariableList = np.append(theVariableList, theYearList)\n",
    "\t\t#print(\"theVariableList: \", theVariableList.shape[0], theVariableList)\n",
    "\t\t\n",
    "\t\ttheVariableZscore = stats.zscore(theVariableList)                      # calculate zscore\n",
    "\t\t#print(\"theVariableZscore: \", theVariableZscore.shape[0], theVariableZscore)\n",
    "\t\t\n",
    "\t\tfor y, year in enumerate(years):\n",
    "\t\t\ti = y * clusters_flattened.shape[0]\n",
    "\t\t\tfor j, row in clusters_flattened.iterrows():\n",
    "\t\t\t\tcluster = row[str(year)]\n",
    "\t\t\t\t#print(\"zValue[%d][%d] += theVariableZscore[%d]\" % (cluster, v, i+j))\n",
    "\t\t\t\tzValue[cluster][v] += theVariableZscore[i+j]                   # accumulate zscore to the position of cluster\n",
    "\t\t\t\tzCount[cluster][v] += 1                                        # count up by 1 to the position of cluster\n",
    "\t#print(zValue)\n",
    "\t#print(zCount)\n",
    "\t\n",
    "\t# calculate average of zscore\n",
    "\tzScore = [[0 for v in range(len(variables))] for c in range(nGeneratedClusters)]\n",
    "\tfor v, variable in enumerate(variables):\n",
    "\t\tfor c in range(nGeneratedClusters):\n",
    "\t\t\tif (zCount[c][v] != 0): zScore[c][v] = round(zValue[c][v] / zCount[c][v], 2)\n",
    "\t#print(zScore)\n",
    "\n",
    "\t# write df_pivot to GEO_VARIABLES.js\n",
    "\tfilename_GEO_VARIABLES = \"NAM_\" + param['filename_suffix'] + \"/data/GEO_VARIABLES_\"+param['filename_suffix']+\".js\"\n",
    "\tofile = open(filename_GEO_VARIABLES, 'w')\n",
    "\tofile.write('var GEO_VARIABLES =\\n')\n",
    "\tofile.write('[\\n')\n",
    "\t#heading = [geoid, 'INC']\n",
    "\t#if (len(years) <= 1): heading = [geoid]\n",
    "\t#heading.extend(list(map(str, years)))\n",
    "\theading = [geoid]\n",
    "\theading.extend(list(map(str, df_pivot.columns.tolist())))\n",
    "\tofile.write('  '+json.dumps(heading)+',\\n')\n",
    "\twCount = 0\n",
    "\tfor i, row in df_pivot.reset_index().iterrows():\n",
    "\t\taLine = row.tolist()\n",
    "\t\tfor j, col in enumerate(aLine[2:], 2):\n",
    "\t\t\ttry:\n",
    "\t\t\t\taLine[j] = int(col)                                  # convert float to int\n",
    "\t\t\texcept ValueError:\n",
    "\t\t\t\taLine[j] = -9999                                     # if Nan, set -9999\n",
    "\t\twCount += 1 \n",
    "\t\tofile.write('  '+json.dumps(aLine)+',\\n')\n",
    "\t#print(\"GEO_VARIABLES.js write count:\", wCount)\n",
    "\tofile.write(']\\n')\n",
    "\t\n",
    "\t# write zscore to GEO_VARIABLES.js\n",
    "\tofile.write('\\n')\n",
    "\tofile.write('var GEO_ZSCORES =\\n')\n",
    "\tofile.write('{\\n')\n",
    "\tofile.write('  \"xAxis\": [\\n')\n",
    "\tfor v, variable in enumerate(labels):\n",
    "\t\tofile.write('    \"'+variable+'\",\\n')\n",
    "\tofile.write('  ],\\n')\n",
    "\tofile.write('  \"yAxis\": '+json.dumps([\"C\"+str(c) for c in range(nGeneratedClusters)])+',\\n')\n",
    "\tofile.write('  \"data\" : [\\n')\n",
    "\tfor z, row in enumerate(zScore):\n",
    "\t\tofile.write('    '+json.dumps(row)+',\\n')\n",
    "\tofile.write('  ],\\n')\n",
    "\tofile.write('}\\n')\n",
    "\n",
    "\tofile.close()\n",
    "\n",
    "def write_ALL_METROS_VARIABLES_js(metros, param):\n",
    "\tgeoid       = metros.columns[0]\n",
    "\tmethod      = param['method']\n",
    "\tnClusters   = param['nClusters']\n",
    "\tyears       = param['years']\n",
    "\tvariables   = param['variables']\n",
    "\tseqClusters = 5\n",
    "\tdistType    = 'tran'\n",
    "\t\n",
    "\tif ('Sequence' in param and type(param['Sequence']) is dict):\n",
    "\t\tif ('seq_clusters' in param['Sequence']): seqClusters = param['Sequence']['seq_clusters']\n",
    "\t\tif ('dist_type' in param['Sequence']): distType = param['Sequence']['dist_type']\n",
    "\t\n",
    "\tprintProgressBar(-1, len(metros.index), prefix = 'Progress:', suffix = 'Initializing', length = 50)\n",
    "\t\n",
    "\tstates = datasets.states(convert=False)\t\t\t\t\t\t\t\t# [56 rows x 3 columns]\n",
    "\t#print(states)\n",
    "\t#print(states[\"geoid\"].tolist())\n",
    "\t\n",
    "\tcounties = datasets.counties(convert=False)\t\t\t\t\t\t\t# [3233 rows x 2 columns]\n",
    "\t#print(counties)\n",
    "\t\n",
    "\tltdb = datasets.ltdb\t\t\t\t\t\t\t\t\t\t\t\t\t# [330388 rows x 192 columns]\n",
    "\t#for column in ltdb.columns:\n",
    "\t#\tprint(column)\n",
    "\t#print(ltdb.memory_usage(index=True, deep=True).sum())\n",
    "\t#print(ltdb)\n",
    "\t\n",
    "\tmsas = datasets.msa_definitions\t\t\t\t\t\t\t\t\t\t# [1915 rows x 13 columns]\n",
    "\t#for column in msas.columns:\n",
    "\t#\tprint(column)\n",
    "\tmsas.set_index('stcofips', inplace=True)\n",
    "\t#print(msas)\n",
    "\t#print(msas.loc['48505', 'CBSA Code'])\n",
    "\t\n",
    "\tcommunity = Community.from_ltdb(years=years, state_fips=states[\"geoid\"].tolist())\n",
    "\t#community = Community.from_ltdb(state_fips=states[\"geoid\"].tolist())\t# [330388 rows x 194 columns]\n",
    "\t#community = Community.from_ltdb(years=years)\n",
    "\t#community.gdf = community.gdf[['geoid', 'year']]\n",
    "\t#for column in community.gdf.columns:\n",
    "\t#\tprint(column)\n",
    "\t\n",
    "\t#community.gdf['metroid'] = None\n",
    "\tmetroids = []\n",
    "\tfor index, row in community.gdf.iterrows():\n",
    "\t\tstcofips = row['geoid'][:5]\n",
    "\t\ttry:\n",
    "\t\t\tmetroids.append(msas.loc[stcofips, 'CBSA Code'])\n",
    "\t\texcept KeyError:\n",
    "\t\t\tmetroids.append(None)\n",
    "\tcommunity.gdf.insert(0, \"stcofips\", metroids, True)\n",
    "\t#print(community.gdf.memory_usage(index=True, deep=True).sum())\n",
    "\t#print(community.gdf)\n",
    "\t#print(variables)\n",
    "\t#print(['geoid', 'year'] + variables)\n",
    "\t#print(variables)\n",
    "\t\n",
    "\tallgdf = community.gdf[['stcofips', 'geoid', 'year'] + variables].copy()\n",
    "\tallgdf.set_index('stcofips', inplace=True)\n",
    "\t#print(allgdf.memory_usage(index=True, deep=True).sum())\n",
    "\t#print(allgdf)\n",
    "\t\n",
    "\t#community.gdf = allgdf.loc['10220']\n",
    "\t#clusters = community.cluster(columns=variables, method=method, n_clusters=nClusters)\n",
    "\t#print(clusters.gdf)\n",
    "\t\n",
    "\t# Initial call to print 0% progress\n",
    "\tprintProgressBar(0, len(metros.index), prefix = 'Progress:', suffix = 'Complete      ', length = 50)\n",
    "\t\n",
    "\treadCount = 0\n",
    "\toutList = []\n",
    "\tfor index, metro in metros.iterrows():\n",
    "\t\t#if (index > 10): break\n",
    "\t\t#if (index != 3): continue\n",
    "\t\t#print(index, metro['geoid'], metro['name'])\n",
    "\t\tmetroid = metro['geoid']\n",
    "\t\tp = metro['name'].rfind(', ')\n",
    "\t\t#if (p < 0): print(index, metro['geoid'], metro['name'], p)\n",
    "\t\tmetroname = metro['name'][:p]\n",
    "\t\tstateabbr = metro['name'][p+2:]\n",
    "\t\t#print(index, metroid, stateabbr, metroname)\n",
    "\t\t\n",
    "\t\t#msa_fips = metroid\n",
    "\t\t#fips_list = []\n",
    "\t\t#if msa_fips:\n",
    "\t\t#\tfips_list += data_store.msa_definitions[\n",
    "\t\t#\t\tdata_store.msa_definitions[\"CBSA Code\"] == msa_fips\n",
    "\t\t#\t][\"stcofips\"].tolist()\n",
    "\t\t##print(metroid, fips_list)\n",
    "\t\t#\n",
    "\t\t#dfs = []\n",
    "\t\t#for fips_code in fips_list:\n",
    "\t\t#\t#dfs.append(data[data.geoid.str.startswith(index)])\n",
    "\t\t#\tdfs.append(allgdf[allgdf.geoid.str.startswith(fips_code)])\n",
    "\t\t#if (len(dfs) == 0): continue\n",
    "\t\t##print(type(dfs))\n",
    "\t\t##print(dfs)\n",
    "\t\t\n",
    "\t\ttry:\n",
    "\t\t\t#community = Community.from_ltdb(years=years, msa_fips=metroid)\n",
    "\t\t\tcommunity.gdf = allgdf.loc[metroid]\n",
    "\t\t\t#community.gdf = allgdf.loc[allgdf['stcofips']==metroid]\n",
    "\t\t\t#community.gdf = pd.concat(dfs)\n",
    "\t\t\t#print(community.gdf)\n",
    "\t\t#except ValueError:\n",
    "\t\texcept KeyError:\n",
    "\t\t\tcontinue\n",
    "\t\t#printProgressBar(index, len(metros.index), prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
    "\t\t#continue\n",
    "\t\t\n",
    "\t\tif (len(community.gdf.index) <= 0): continue\n",
    "\t\t#print(community.gdf.columns)\n",
    "\t\t#for column in community.gdf.columns:\n",
    "\t\t#\tprint(column)\n",
    "\t\t#print(community)\n",
    "\t\t#community.gdf.to_csv(r'output.csv')\n",
    "\t\t#print(community.gdf)\n",
    "\t\t#print(\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\")\t\t\n",
    "\t\t# clustering by method, nClusters with filtering by variables\n",
    "\t\t#try:\n",
    "\t\tif (method == 'kmeans' or method == 'ward' or method == 'affinity_propagation' or method == 'spectral' or method == 'gaussian_mixture' or method == 'hdbscan'):\n",
    "\t\t\ttry:\n",
    "\t\t\t\tclusters = community.cluster(columns=variables, method=method, n_clusters=nClusters)\n",
    "\t\t\texcept KeyError:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t\t\n",
    "\t\tif (method == 'ward_spatial' or method == 'spenc' or method == 'skater' or method == 'azp' or method == 'max_p'):\n",
    "\t\t\ttry:\n",
    "\t\t\t\tclusters = community.cluster_spatial(columns=variables, method=method, n_clusters=nClusters) #, spatial_weights='rook'\n",
    "\t\t\texcept KeyError:\n",
    "\t\t\t\tcontinue\n",
    "\t\t#except KeyError:\n",
    "\t\t#\tcontinue\n",
    "\t\t#print(clusters.gdf)\n",
    "\t\t#print(clusters.gdf[['year', 'geoid', 'kmeans']])\n",
    "\t\t\n",
    "\t\t# get pivot from clusters\n",
    "\t\tdf_pivot = clusters.gdf.pivot(index='geoid', columns='year', values='kmeans')\n",
    "\t\t#print(df_pivot)\n",
    "\t\t#print(len(df_pivot.index))\n",
    "\t\t#print(df_pivot.columns)\n",
    "\t\t\n",
    "\t\tif (len(df_pivot.columns) > 1):\t\t\t\t\t\t\t\t\t\t# 1980, 1990, 2000, 2010\n",
    "\t\t\t# convert df_pivot to list for INCS.linc\n",
    "\t\t\tyearList = []\n",
    "\t\t\tfor year in df_pivot.columns:\n",
    "\t\t\t\taYearList = df_pivot[year].values.tolist()\n",
    "\t\t\t\taYearList = list(map(float, aYearList)) \n",
    "\t\t\t\tyearList.append(aYearList)\n",
    "\t\t\t#print(yearList)\n",
    "\t\t\t# calculate INC\n",
    "\t\t\tincs = linc(yearList)\n",
    "\t\t\t#print(incs)\n",
    "\t\t\tave = sum(incs) / len(incs) if (len(incs) != 0) else -9999\n",
    "\t\t\t#print(\"ave:\", ave)\n",
    "\t\t\t#print(index, metroid, ave, stateabbr, metroname)\n",
    "\t\t\treadCount += len(incs)\n",
    "\t\t\toutList.append([metroid, ave])\n",
    "\t\t\tprintProgressBar(index, len(metros.index), prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
    "\tprintProgressBar(len(metros.index), len(metros.index), prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
    "\t#print(outList)\n",
    "\t#print(\"readCount2:\", readCount)\n",
    "\t\n",
    "\t# write df_pivot to GEO_VARIABLES.js\n",
    "\tfilename_GEO_VARIABLES = \"NAM_\" + param['filename_suffix'] + \"/data/GEO_VARIABLES_\"+param['filename_suffix']+\".js\"\n",
    "\tofile = open(filename_GEO_VARIABLES, 'w')\n",
    "\tofile.write('var GEO_VARIABLES =\\n')\n",
    "\tofile.write('[\\n')\n",
    "\theading = [geoid, 'INC']\n",
    "\tofile.write('  '+json.dumps(heading)+',\\n')\n",
    "\twCount = 0\n",
    "\tfor i, row in enumerate(outList):\n",
    "\t\twCount += 1\n",
    "\t\tofile.write('  '+json.dumps(row)+',\\n')\n",
    "\t#print(\"GEO_VARIABLES.js write count:\", wCount)\n",
    "\tofile.write(']\\n')\n",
    "\tofile.close()\n",
    "\n",
    "\n",
    "def Clustering_viz(param):\n",
    "\twrite_LOG(param)\n",
    "\t\n",
    "\t# select community by state_fips, msa_fips, county_fips\n",
    "\tmetros = None\n",
    "\tcommunity = None\n",
    "\tif ('allMetros' in param and param['allMetros']):\n",
    "\t\t#metros = data_store.msa_definitions\n",
    "\t\t#print(metros)\n",
    "\t\t#metros = data_store.msas()\n",
    "\t\tmetros = datasets.msas()\n",
    "\telif ('msa_fips' in param and param['msa_fips']):\n",
    "\t\tcommunity = Community.from_ltdb(years=param['years'], msa_fips=param['msa_fips'])\n",
    "\t\t#community = Community.from_ltdb(msa_fips=param['msa_fips'])\n",
    "\telif ('county_fips' in param and param['county_fips']):\n",
    "\t\tcommunity = Community.from_ltdb(years=param['years'], county_fips=param['county_fips'])\n",
    "\telif ('state_fips' in param and param['state_fips']):\n",
    "\t\tcommunity = Community.from_ltdb(years=param['years'], state_fips=param['state_fips'])\n",
    "\t\n",
    "\tcommunity.gdf = community.gdf.replace([np.inf, -np.inf], np.nan)\n",
    "\t\n",
    "\t# check if geometry is not null for Spatial Clustering\n",
    "\tcommunity.gdf = community.gdf[pd.notnull(community.gdf['geometry'])]\n",
    "\t#print(community.gdf)\n",
    "    \n",
    "\tcodebook = pd.read_csv('template/conversion_table_codebook.csv')\n",
    "\tcodebook.set_index(keys='variable', inplace=True)\n",
    "\tlabels = copy.deepcopy(param['variables'])\n",
    "\tlabel = 'short_name'                                             # default\n",
    "\tif ('label' in param and param['label'] == 'variable'): label = 'variable'\n",
    "\tif ('label' in param and param['label'] == 'full_name'): label = 'full_name'\n",
    "\tif ('label' in param and param['label'] == 'short_name'): label = 'short_name'\n",
    "\tif (label != 'variable'):\n",
    "\t\tfor idx, variable in enumerate(param['variables']):\n",
    "\t\t\ttry:\n",
    "\t\t\t\tcodeRec = codebook.loc[variable]\n",
    "\t\t\t\tlabels[idx] = codeRec[label]\n",
    "\t\t\texcept:\n",
    "\t\t\t\tprint(\"variable not found in codebook.  variable:\", variable)\n",
    "\tparam['labels'] = labels\n",
    "    \n",
    "\tif (community):\n",
    "\t\twrite_INDEX_html(param)\n",
    "\t\twrite_GEO_CONFIG_js(param)\n",
    "\t\twrite_GEO_VARIABLES_js(community, param)\n",
    "\t\twrite_GEO_JSON_js(community, param)\n",
    "\t\n",
    "\tif (metros is not None):\n",
    "\t\twrite_ALL_METROS_INDEX_html(param)\n",
    "\t\twrite_ALL_METROS_GEO_CONFIG_js(param)\n",
    "\t\twrite_ALL_METROS_VARIABLES_js(metros, param)\n",
    "\t\twrite_ALL_METROS_JSON_js(metros, param)\n",
    "\t\n",
    "\t#local_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "\tservers = list(notebookapp.list_running_servers())\n",
    "\tservers1 = 'https://cybergisx.cigi.illinois.edu'+servers[0][\"base_url\"]+ 'view'\n",
    "\tservers2 = 'https://cybergisx.cigi.illinois.edu'+servers[0][\"base_url\"]+ 'edit'      \n",
    "\tcwd = os.getcwd()\n",
    "\tprefix_cwd = \"/home/jovyan/work\"\n",
    "\tcwd = cwd.replace(prefix_cwd, \"\")\n",
    "\tlocal_dir1 = servers1 + cwd\n",
    "\tlocal_dir2 = servers2 + cwd    \n",
    "\t#print(local_dir)\n",
    "\tfname =urllib.parse.quote('index.html')\n",
    "\ttemplate_dir = os.path.join(local_dir1, 'NAM_' + param['filename_suffix'])\n",
    "\t#url = 'file:' + os.path.join(template_dir, fname)\n",
    "\turl = os.path.join(template_dir, fname)    \n",
    "\twebbrowser.open(url)\n",
    "\tprint('To see visualization of your analysis, click the URL below:')\n",
    "\tprint(url)    \n",
    "\tprint('Advanced options are available in ')  \n",
    "\tprint(local_dir2 + '/'+ 'NAM_' + param['filename_suffix']+'/data/GEO_CONFIG_' + param['filename_suffix']+'.js')  \n",
    "    \n",
    "def Clustering_log():\n",
    "\t# build array of logs from directory of 'NAM_'\n",
    "\tlogs = []\n",
    "\tdirname = os.getcwd()\n",
    "\tsubnames = os.listdir(dirname)\n",
    "\tfor subname in subnames:\n",
    "\t\tfullpath = os.path.join(dirname, subname)\n",
    "\t\tif (not os.path.isdir(fullpath)): continue\n",
    "\t\tif (not subname.startswith('NAM_')): continue\n",
    "\t\t#print(os.path.join(fullpath, 'index.html'))\n",
    "\t\tindexfile = os.path.join(fullpath, 'index.html')\n",
    "\t\tlogfile = os.path.join(fullpath, 'data/param.log')\n",
    "\t\tif (not os.path.exists(indexfile)): continue\n",
    "\t\tif (not os.path.exists(logfile)): continue\n",
    "\t\t#print(fullpath, logfile)\n",
    "\t\t# read param.log\n",
    "\t\tifile = open(logfile, \"r\", encoding=\"utf-8\")\n",
    "\t\twholetext = ifile.read()\n",
    "\t\tcontents = wholetext.split('\\n', maxsplit=1)\n",
    "\t\tif (len(contents) != 2): continue\n",
    "\t\tcreate_at = contents[0]\n",
    "\t\tparam     = contents[1]\n",
    "\t\t#print(create_at)\n",
    "\t\t#print(param)\n",
    "\t\tlogs.append({'indexfile': os.path.join(subname, 'index.html'), 'create_at': create_at, 'param': param})\n",
    "\tlogs = sorted(logs, key=lambda k: k['create_at']) \n",
    "\t#print(logs)\n",
    "\t\n",
    "\t#Write output to log.html\n",
    "\tfilename_LOG = \"log.html\"\n",
    "\tofile = open(filename_LOG, 'w')\n",
    "\tofile.write('<!DOCTYPE html>\\n')\n",
    "\tofile.write('<html>\\n')\n",
    "\tofile.write('<head>\\n')\n",
    "\tofile.write('  <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\"/>\\n')\n",
    "\tofile.write('  <title>Neighborhood Analysis Logging</title>\\n')\n",
    "\tofile.write('</head>\\n')\n",
    "\tofile.write('<body>\\n')\n",
    "\tofile.write('  <header>\\n')\n",
    "\tofile.write('    <h1>Neighborhood Analysis Logging</h1>\\n')\n",
    "\tofile.write('  </header>\\n')\n",
    "\t\n",
    "\tfor idx, val in enumerate(logs):\n",
    "\t\tparams = val['param'].split('\\n')\n",
    "\t\thtml = '\\n'\n",
    "\t\thtml += '<div style=\"margin:10px; float:left; border: 1px solid #99CCFF; border-radius: 5px;\">\\n'\n",
    "\t\thtml += '  <table>\\n'\n",
    "\t\thtml += '    <tr>\\n'\n",
    "\t\thtml += '      <td>\\n'\n",
    "\t\thtml += '        <button id=\"global_submit\" type=\"button\" style=\"margin:0px 20px 0px 5px;\" onclick=\"window.open(\\'' + val['indexfile'] + '\\')\">' + str(idx+1) + '. Show This</button>\\n'\n",
    "\t\thtml += '        ' + val['create_at'] + '\\n'\n",
    "\t\thtml += '      </td>\\n'\n",
    "\t\thtml += '    </tr>\\n'\n",
    "\t\thtml += '    <tr>\\n'\n",
    "\t\thtml += '      <td>\\n'\n",
    "\t\thtml += '<pre>\\n'\n",
    "\t\tfor param in params:\n",
    "\t\t\thtml += param + '\\n'\n",
    "\t\thtml += '</pre>\\n'\n",
    "\t\thtml += '      </td>\\n'\n",
    "\t\thtml += '    </tr>\\n'\n",
    "\t\thtml += '  </table>\\n'\n",
    "\t\thtml += '</div>\\n'\n",
    "\t\tofile.write(html)\n",
    "\t\n",
    "\tofile.write('</body>\\n')\n",
    "\tofile.write('</html>')\n",
    "\tofile.close()\n",
    "\t\n",
    "\tlocal_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "\tfname =urllib.parse.quote(filename_LOG)\n",
    "\turl = 'file:' + os.path.join(local_dir, fname)\n",
    "\twebbrowser.open(url)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tstarted_datetime = datetime.now()\n",
    "\tdateYYMMDD = started_datetime.strftime('%Y%m%d')\n",
    "\ttimeHHMMSS = started_datetime.strftime('%H%M%S')\n",
    "\tprint('GEOSNAP2NAM start at %s %s' % (started_datetime.strftime('%Y-%m-%d'), started_datetime.strftime('%H:%M:%S')))\n",
    "\t\n",
    "\t#sample = \"downloads/LTDB_Std_All_Sample.zip\"\n",
    "\t#full = \"downloads/LTDB_Std_All_fullcount.zip\"\n",
    "\t#store_ltdb(sample=sample, fullcount=full)\n",
    "\tstore_census()\n",
    "    \n",
    "\tended_datetime = datetime.now()\n",
    "\telapsed = ended_datetime - started_datetime\n",
    "\ttotal_seconds = int(elapsed.total_seconds())\n",
    "\thours, remainder = divmod(total_seconds,60*60)\n",
    "\tminutes, seconds = divmod(remainder,60)\t\n",
    "\tprint('GEOSNAP2NAM ended at %s %s    Elapsed %02d:%02d:%02d' % (ended_datetime.strftime('%Y-%m-%d'), ended_datetime.strftime('%H:%M:%S'), hours, minutes, seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pyproj/crs/crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/cluster/hierarchy.py:830: ClusterWarning: scipy.cluster: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix\n",
      "  return linkage(y, method='ward', metric='euclidean')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To see visualization of your analysis, click the URL below:\n",
      "https://cybergisx.cigi.illinois.edu/user/suhanmappingideas/view/geosnap-viz/GEOSNAP2NAM/NAM_SD_everything_3/index.html\n",
      "Advanced options are available in \n",
      "https://cybergisx.cigi.illinois.edu/user/suhanmappingideas/edit/geosnap-viz/GEOSNAP2NAM/NAM_SD_everything_3/data/GEO_CONFIG_SD_everything_3.js\n"
     ]
    }
   ],
   "source": [
    "param = {\n",
    "    'title': \"Neighborhood Analysis: Kmeans, San Diego\",\n",
    "    'filename_suffix': \"SD_everything_3\",              # \"Albertville\"\n",
    "    'state_fips': None,\n",
    "    'msa_fips': \"41740\",                         # \"10700\"\n",
    "    'county_fips': None,\n",
    "    'years': [1980, 1990, 2000, 2010],           # Available years: 1970, 1980, 1990, 2000 and 2010\n",
    "    'method': \"kmeans\",                          # Aspatial Clustering: affinity_propagation, gaussian_mixture, hdbscan, kmeans, spectral, ward\n",
    "                                                 # Spatial Clustering: azp, max_p, skater, spenc, ward_spatial   \n",
    "    'nClusters': 8,                              # This option should be commented out for affinity_propagation, hdbscan and max_p \n",
    "    'variables': [\"p_nonhisp_white_persons\", \n",
    "                  \"p_nonhisp_black_persons\", \n",
    "                  \"p_hispanic_persons\", \n",
    "                  \"p_native_persons\", \n",
    "                  \"p_asian_persons\",\n",
    "                 ],\n",
    "    'Sequence': {'seq_clusters': 5, 'dist_type': 'tran'},    # seq_clusters': 5, dist_type': 'tran'\n",
    "    # optional visualization below.\n",
    "    'Index_of_neighborhood_change': True,        #choropleth map: Maps representing index of neighborhood Change\n",
    "    'Maps_of_neighborhood': True,                #choropleth map: Maps representing clustering result\t\t\n",
    "    'Distribution_INC1': True,                   #density chart: INC changes as the map extent changes \n",
    "    'Distribution_INC2_different_period': True,  #density chart: INC changes by different years\n",
    "    'Distribution_INC2_different_cluster': True, #density chart: INC changes by different clusters\n",
    "    'Temporal_change_in_neighborhoods': True,    #stacked chart: Temporal Change in Neighborhoods over years\t\t\n",
    "    'Parallel_Categories_Diagram_in_neighborhoods': True,\n",
    "    'Chord_Diagram_in_neighborhoods': True, \n",
    "}\n",
    "\n",
    "Clustering_viz(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
